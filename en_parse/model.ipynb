{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "PAD = 0\n",
    "EOS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[2, 4, 8],\n",
    "     [2, 5, 9],\n",
    "     [3, 6, 0],\n",
    "     [0, 7, 0]]\n",
    "xl = [3, 4, 2]\n",
    "\n",
    "encoder_inputs = tf.constant(x, dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.constant(xl, dtype=tf.int32, name='encoder_inputs_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "vocab_size = 10\n",
    "embedding_size = 5\n",
    "input_hidden_unit = 20\n",
    "\n",
    "with tf.variable_scope(\"embedding\") as scope:\n",
    "    encoder_cell = tf.contrib.rnn.LSTMCell(input_hidden_unit)\n",
    "    sqrt3 = math.sqrt(3)\n",
    "    initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "\n",
    "    embedding_matrix = tf.get_variable(\n",
    "        name=\"embedding_matrix\",\n",
    "        shape=[vocab_size, embedding_size],\n",
    "        initializer=initializer,\n",
    "        dtype=tf.float32)\n",
    "\n",
    "    embedded_encoder_inputs = tf.nn.embedding_lookup(\n",
    "        embedding_matrix, encoder_inputs)\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(\n",
    "        cell=encoder_cell,\n",
    "        inputs=embedded_encoder_inputs,\n",
    "        sequence_length=encoder_inputs_length,\n",
    "        time_major=True,\n",
    "        dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding/rnn/while/Exit_2:0' shape=(3, 20) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_hidden_unit = 10\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.LSTMCell(output_hidden_unit)\n",
    "helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "    embedding_decoder, start_tokens, end_token)\n",
    "\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    cell=cell,\n",
    "    helper=helper,\n",
    "    initial_state=cell.zero_state(batch_size, tf.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_hidden_unit = 20\n",
    "output_hidden_unit = 10\n",
    "\n",
    "encoder_cell = tf.contrib.rnn.LSTMCell(input_hidden_unit)\n",
    "decoder_cell = tf.contrib.rnn.LSTMCell(output_hidden_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[5, 6, 7],\n",
    "     [7, 6, 8],\n",
    "     [3, 8, 0],\n",
    "     [0, 7, 0]]\n",
    "xl = [3, 4, 2]\n",
    "encoder_inputs = tf.constant(x, dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.constant(xl, dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "decoder_targets = tf.constant(x, dtype=tf.int32, name='decoder_targets')\n",
    "decoder_targets_length = tf.constant(xl, dtype=tf.int32, name='decoder_targets_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('DecoderTrainFeeds'):\n",
    "    sequence_size, batch_size = tf.unstack(tf.shape(decoder_targets))\n",
    "\n",
    "    EOS_SLICE = tf.ones([1, batch_size], dtype=tf.int32) * EOS\n",
    "    PAD_SLICE = tf.ones([1, batch_size], dtype=tf.int32) * PAD\n",
    "\n",
    "    decoder_train_inputs = tf.concat([EOS_SLICE, decoder_targets], axis=0)\n",
    "    decoder_train_length = decoder_targets_length + 1\n",
    "\n",
    "    decoder_train_targets = tf.concat([decoder_targets, PAD_SLICE], axis=0)\n",
    "    decoder_train_targets_seq_len, _ = tf.unstack(tf.shape(decoder_train_targets))\n",
    "    decoder_train_targets_eos_mask = tf.one_hot(\n",
    "        decoder_train_length - 1,\n",
    "        decoder_train_targets_seq_len,\n",
    "        on_value=EOS,\n",
    "        off_value=PAD,\n",
    "        dtype=tf.int32\n",
    "    )\n",
    "    decoder_train_targets_eos_mask = tf.transpose(\n",
    "        decoder_train_targets_eos_mask, [1, 0])\n",
    "\n",
    "    # hacky way using one_hot to put EOS symbol at the end of target sequence\n",
    "    decoder_train_targets = tf.add(decoder_train_targets,\n",
    "                                   decoder_train_targets_eos_mask)\n",
    "\n",
    "    loss_weights = tf.ones([\n",
    "        batch_size,\n",
    "        tf.reduce_max(decoder_train_length)\n",
    "    ], dtype=tf.float32, name=\"loss_weights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "vocab_size = 10\n",
    "embedding_size = 5\n",
    "\n",
    "with tf.variable_scope(\"embeddingss\") as scope:\n",
    "    sqrt3 = math.sqrt(3)\n",
    "    initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "\n",
    "    embedding_matrix = tf.get_variable(\n",
    "        name=\"embedding_matrix\",\n",
    "        shape=[vocab_size, embedding_size],\n",
    "        initializer=initializer,\n",
    "        dtype=tf.float32)\n",
    "\n",
    "    encoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "        embedding_matrix, encoder_inputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\") as scope:\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "        cell=encoder_cell,\n",
    "        inputs=encoder_inputs_embedded,\n",
    "        sequence_length=encoder_inputs_length,\n",
    "        time_major=True,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adagrad': tensorflow.python.training.adagrad.AdagradOptimizer,\n",
       " 'Adam': tensorflow.python.training.adam.AdamOptimizer,\n",
       " 'Ftrl': tensorflow.python.training.ftrl.FtrlOptimizer,\n",
       " 'Momentum': tensorflow.python.training.momentum.MomentumOptimizer,\n",
       " 'RMSProp': tensorflow.python.training.rmsprop.RMSPropOptimizer,\n",
       " 'SGD': tensorflow.python.training.gradient_descent.GradientDescentOptimizer}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.contrib.layers.OPTIMIZER_CLS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 2], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(encoder_inputs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell=deocder_cell,\n",
    "        attention_mechanism=attention_mechanism,\n",
    "        name='Attention_Wrapper'\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Decoder\") as scope:\n",
    "    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "        num_units=input_hidden_unit,\n",
    "        memory=encoder_outputs\n",
    "    )\n",
    "    \n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell=decoder_cell,\n",
    "        attention_mechanism=attention_mechanism,\n",
    "        name='Attention_Wrapper'\n",
    "    )\n",
    "    \n",
    "    def output_fn(outputs):\n",
    "        return tf.contrib.layers.linear(outputs, self.vocab_size, scope=scope)\n",
    "\n",
    "    if not attention:\n",
    "        decoder_fn_train = seq2seq.simple_decoder_fn_train(encoder_state=encoder_state)\n",
    "        decoder_fn_inference = seq2seq.simple_decoder_fn_inference(\n",
    "            output_fn=output_fn,\n",
    "            encoder_state=encoder_state,\n",
    "            embeddings=embedding_matrix,\n",
    "            start_of_sequence_id=self.EOS,\n",
    "            end_of_sequence_id=self.EOS,\n",
    "            maximum_length=tf.reduce_max(self.encoder_inputs_length) + 3,\n",
    "            num_decoder_symbols=self.vocab_size,\n",
    "        )\n",
    "    else:\n",
    "        # attention_states: size [batch_size, max_time, num_units]\n",
    "        attention_states = tf.transpose(self.encoder_outputs, [1, 0, 2])\n",
    "\n",
    "        (attention_keys,\n",
    "        attention_values,\n",
    "        attention_score_fn,\n",
    "        attention_construct_fn) = seq2seq.prepare_attention(\n",
    "            attention_states=attention_states,\n",
    "            attention_option=\"bahdanau\",\n",
    "            num_units=self.decoder_hidden_units,\n",
    "        )\n",
    "\n",
    "        decoder_fn_train = seq2seq.attention_decoder_fn_train(\n",
    "            encoder_state=self.encoder_state,\n",
    "            attention_keys=attention_keys,\n",
    "            attention_values=attention_values,\n",
    "            attention_score_fn=attention_score_fn,\n",
    "            attention_construct_fn=attention_construct_fn,\n",
    "            name='attention_decoder'\n",
    "        )\n",
    "\n",
    "        decoder_fn_inference = seq2seq.attention_decoder_fn_inference(\n",
    "            output_fn=output_fn,\n",
    "            encoder_state=self.encoder_state,\n",
    "            attention_keys=attention_keys,\n",
    "            attention_values=attention_values,\n",
    "            attention_score_fn=attention_score_fn,\n",
    "            attention_construct_fn=attention_construct_fn,\n",
    "            embeddings=self.embedding_matrix,\n",
    "            start_of_sequence_id=self.EOS,\n",
    "            end_of_sequence_id=self.EOS,\n",
    "            maximum_length=tf.reduce_max(self.encoder_inputs_length) + 3,\n",
    "            num_decoder_symbols=self.vocab_size,\n",
    "        )\n",
    "\n",
    "    (self.decoder_outputs_train,\n",
    "     self.decoder_state_train,\n",
    "     self.decoder_context_state_train) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "            cell=self.decoder_cell,\n",
    "            decoder_fn=decoder_fn_train,\n",
    "            inputs=self.decoder_train_inputs_embedded,\n",
    "            sequence_length=self.decoder_train_length,\n",
    "            time_major=True,\n",
    "            scope=scope,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    self.decoder_logits_train = output_fn(self.decoder_outputs_train)\n",
    "    self.decoder_prediction_train = tf.argmax(self.decoder_logits_train, axis=-1, name='decoder_prediction_train')\n",
    "\n",
    "    scope.reuse_variables()\n",
    "\n",
    "    (self.decoder_logits_inference,\n",
    "     self.decoder_state_inference,\n",
    "     self.decoder_context_state_inference) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "            cell=self.decoder_cell,\n",
    "            decoder_fn=decoder_fn_inference,\n",
    "            time_major=True,\n",
    "            scope=scope,\n",
    "        )\n",
    "    )\n",
    "    self.decoder_prediction_inference = tf.argmax(self.decoder_logits_inference, axis=-1, name='decoder_prediction_inference')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_inputs_embedded,\n",
    "    dtype=tf.float32, time_major=True,\n",
    ")\n",
    "\n",
    "del encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "    decoder_cell, decoder_inputs_embedded,\n",
    "\n",
    "    initial_state=encoder_final_state,\n",
    "\n",
    "    dtype=tf.float32, time_major=True, scope=\"plain_decoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size)\n",
    "\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ = [[6], [3, 4], [9, 8, 7]]\n",
    "\n",
    "batch_, batch_length_ = batch(batch_)\n",
    "print('batch_encoded:\\n' + str(batch_))\n",
    "\n",
    "din_, dlen_ = batch(np.ones(shape=(3, 1), dtype=np.int32),\n",
    "                            max_sequence_length=4)\n",
    "print('decoder inputs:\\n' + str(din_))\n",
    "\n",
    "pred_ = sess.run(decoder_prediction,\n",
    "    feed_dict={\n",
    "        encoder_inputs: batch_,\n",
    "        decoder_inputs: din_,\n",
    "    })\n",
    "print('decoder predictions:\\n' + str(pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
